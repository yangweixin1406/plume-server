import os
import json
import pymysql
from datetime import datetime, timedelta
from tqdm import tqdm
import time

# ================= æ•°æ®åº“é…ç½® =================
DB_CONFIG = {
    "host": "127.0.0.1",
    "user": "root",
    "password": "123456",
    "database": "plume_db",
    "port": 3306,
    "charset": "utf8mb4",
    "autocommit": False,
    "connect_timeout": 10
}

BASE_BATCH_SIZE = 2000
MAX_RETRY = 3

# ================= SQL å¸¸é‡ =================
SQL_USER = """
    INSERT INTO users (wallet_address, referred_by, referral_count)
    VALUES (%s,%s,%s)
    ON DUPLICATE KEY UPDATE 
        id=LAST_INSERT_ID(id),
        referred_by=VALUES(referred_by),
        referral_count=VALUES(referral_count)
"""

SQL_SNAPSHOT = """
    INSERT INTO user_snapshots (
        user_id, snapshot_date, bridged_total, swap_volume, swap_count,
        tvl_total_usd, real_tvl_usd, protocols_used, longest_swap_streak_weeks,
        adjustment_points, protectors_points, badge_points, user_self_xp,
        referral_bonus_xp, total_xp, xp_rank, longest_tvl_streak,
        plume_staking_points, plume_staking_bonus, plume_staking_total_tokens
    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE
        bridged_total=VALUES(bridged_total),
        swap_volume=VALUES(swap_volume),
        swap_count=VALUES(swap_count),
        tvl_total_usd=VALUES(tvl_total_usd),
        real_tvl_usd=VALUES(real_tvl_usd),
        protocols_used=VALUES(protocols_used),
        longest_swap_streak_weeks=VALUES(longest_swap_streak_weeks),
        adjustment_points=VALUES(adjustment_points),
        protectors_points=VALUES(protectors_points),
        badge_points=VALUES(badge_points),
        user_self_xp=VALUES(user_self_xp),
        referral_bonus_xp=VALUES(referral_bonus_xp),
        total_xp=VALUES(total_xp),
        xp_rank=VALUES(xp_rank),
        longest_tvl_streak=VALUES(longest_tvl_streak),
        plume_staking_points=VALUES(plume_staking_points),
        plume_staking_bonus=VALUES(plume_staking_bonus),
        plume_staking_total_tokens=VALUES(plume_staking_total_tokens)
"""

SQL_CHANGE = """
    INSERT INTO user_daily_changes (user_id, snapshot_date, xp_change, tvl_change)
    VALUES (%s,%s,%s,%s)
    ON DUPLICATE KEY UPDATE
        xp_change=VALUES(xp_change),
        tvl_change=VALUES(tvl_change)
"""

# ================= å·¥å…·å‡½æ•° =================
def get_connection():
    for attempt in range(MAX_RETRY):
        try:
            return pymysql.connect(**DB_CONFIG)
        except pymysql.MySQLError:
            print(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{MAX_RETRY} â€¦")
            time.sleep(1)
    raise ConnectionError("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

def clean_tvl(value, max_value=1e12):
    """æ¸…ç† tvl æ•°å€¼ï¼Œä¿è¯æ’å…¥æ•°æ®åº“ä¸ä¼šæº¢å‡º"""
    try:
        val = float(value)
    except (ValueError, TypeError):
        return 0
    if val < 0:
        return 0
    if val > max_value:  # é¿å…è¶…å‡ºæ•°æ®åº“èŒƒå›´
        return max_value
    return val

def snapshot_values(user_id, snapshot_date, data):
    tvl_total_usd = data.get("tvlTotalUsd", 0)
    if tvl_total_usd is None:
        tvl_total_usd = 0
    else:
        try:
            tvl_total_usd = float(tvl_total_usd)
        except (ValueError, TypeError):
            tvl_total_usd = 0
    if tvl_total_usd < 0:
        tvl_total_usd = 0
    return (
        user_id,
        snapshot_date,
        data.get("bridgedTotal", 0),
        data.get("swapVolume", 0),
        data.get("swapCount", 0),
        clean_tvl(data.get("tvlTotalUsd", 0)),     # âœ… å·²æ¸…æ´—
        clean_tvl(data.get("realTvlUsd", 0)),      # âœ… å»ºè®® real_tvl_usd ä¹Ÿæ¸…æ´—
        data.get("protocolsUsed", 0),
        data.get("longestSwapStreakWeeks", 0),
        data.get("adjustmentPoints", 0),
        data.get("protectorsOfPlumePoints", 0),
        data.get("badgePoints", 0),
        data.get("userSelfXp", 0),
        data.get("referralBonusXp", 0),
        data.get("totalXp", 0),
        data.get("xpRank"),
        data.get("longestTvlStreak", 0),
        data.get("plumeStakingPointsEarned", 0),
        data.get("plumeStakingBonusPointsEarned", 0),
        data.get("currentPlumeStakingTotalTokens", 0),
    )


def daily_change_values(user_id, snapshot_date, xp_change, tvl_change):
    return (user_id, snapshot_date, xp_change, tvl_change)


def get_batch_size(total_lines):
    if total_lines > 200_000:
        return 4000
    elif total_lines > 100_000:
        return 3000
    else:
        return BASE_BATCH_SIZE

# ================= æ‰¹æ¬¡å¤„ç† =================
def process_batch(batch, attempt=1):
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # ---- æ‰¹é‡è§£æ JSON ----
        parsed = [json.loads(line.strip()) for line in batch if line.strip()]

        # ---- æ‰¹é‡ upsert ç”¨æˆ· ----
        wallets = [(d["walletAddress"], d.get("referredBy"), d.get("referralCount", 0)) for d in parsed]
        cursor.executemany(SQL_USER, wallets)
        conn.commit()  # ç¡®ä¿ id æ˜ å°„å¯ç”¨

        # è·å– user_id æ˜ å°„
        cursor.execute(
            f"SELECT id, wallet_address FROM users WHERE wallet_address IN ({','.join(['%s']*len(wallets))})",
            [w[0] for w in wallets]
        )
        user_map = {w: uid for uid, w in cursor.fetchall()}

        # ---- æ‰¹é‡æŸ¥è¯¢æ˜¨å¤©çš„å¿«ç…§ ----
        snapshot_date = datetime.strptime(parsed[0]["dateStr"].split("_")[0], "%Y-%m-%d").date()
        yesterday = snapshot_date - timedelta(days=1)
        cursor.execute(
            f"SELECT user_id, total_xp, tvl_total_usd FROM user_snapshots "
            f"WHERE snapshot_date=%s AND user_id IN ({','.join(['%s']*len(user_map))})",
            [yesterday] + list(user_map.values())
        )
        yesterday_map = {row[0]: (row[1], row[2]) for row in cursor.fetchall()}

        # ---- ç”Ÿæˆå¿«ç…§ & æ—¥å˜åŒ–æ•°æ® ----
        snapshots_batch, changes_batch = [], []
        for data in parsed:
            user_id = user_map[data["walletAddress"]]

            # å¿«ç…§
            snapshots_batch.append(snapshot_values(user_id, snapshot_date, data))

            # æ—¥å˜åŒ–
            y_xp, y_tvl = yesterday_map.get(user_id, (0, 0))
            xp_change = int(data.get("totalXp", 0)) - int(y_xp)
            tvl_change = float(data.get("tvlTotalUsd", 0)) - float(y_tvl)
            
            # âœ… æ¸…ç† tvl_changeï¼Œé¿å…æº¢å‡º
            tvl_change = clean_tvl(tvl_change)

            if xp_change or tvl_change:
                changes_batch.append(daily_change_values(user_id, snapshot_date, xp_change, tvl_change))

        # ---- æ‰¹é‡æ’å…¥ ----
        if snapshots_batch:
            cursor.executemany(SQL_SNAPSHOT, snapshots_batch)
        if changes_batch:
            cursor.executemany(SQL_CHANGE, changes_batch)

        conn.commit()
        return True

    except pymysql.err.OperationalError as e:
        if attempt < MAX_RETRY and (1213 in str(e) or "MySQL server has gone away" in str(e)):
            print(f"âš ï¸ ç¬¬{attempt}æ¬¡é‡è¯•æ‰¹æ¬¡ï¼ŒåŸå› : {e}")
            time.sleep(0.5 * attempt)
            return process_batch(batch, attempt + 1)
        else:
            return f"âŒ å‡ºé”™: {e}\næ•°æ®ç¤ºä¾‹: {batch[0].strip() if batch else 'ç©º'}"
    except Exception as e:
        return f"âŒ å‡ºé”™: {e}\næ•°æ®ç¤ºä¾‹: {batch[0].strip() if batch else 'ç©º'}"
    finally:
        try:
            cursor.close()
            conn.close()
        except:
            pass

# ================= æ‰¹é‡å¯¼å…¥å…¥å£ =================
def bulk_insert(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    batch_size = get_batch_size(len(lines))
    batches = [lines[i:i + batch_size] for i in range(0, len(lines), batch_size)]
    print(f"ğŸš€ å¼€å§‹å¯¼å…¥ï¼Œæ€»æ•°æ®={len(lines)}, æ‰¹æ¬¡æ•°={len(batches)}, æ‰¹æ¬¡å¤§å°={batch_size}")

    for batch in tqdm(batches, desc="æ’å…¥æ•°æ®"):
        result = process_batch(batch)
        if result is not True:
            print(result)

    print("âœ… å•å¤©å¢é‡æ•°æ®æ’å…¥å®Œæˆ")

# ================= å¹³å°ç»Ÿè®¡ =================
def update_platform_stats(snapshot_date):
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute("""
            SELECT 
                COUNT(*) AS total_wallets,
                SUM(total_xp) AS total_xp
            FROM user_snapshots
            WHERE snapshot_date=%s
              AND xp_rank IS NOT NULL
              AND total_xp <> 0
        """, (snapshot_date,))
        row = cursor.fetchone()
        if not row or row[0] == 0:
            print(f"âš ï¸ {snapshot_date} æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å¿«ç…§æ•°æ®ï¼Œç»Ÿè®¡è·³è¿‡")
            return

        total_wallets, total_xp = row

        yesterday = snapshot_date - timedelta(days=1)
        cursor.execute("""
            SELECT total_wallets, total_xp
            FROM platform_stats
            WHERE snapshot_date=%s
        """, (yesterday,))
        prev = cursor.fetchone()
        if prev:
            prev_wallets, prev_xp = prev
        else:
            prev_wallets, prev_xp = 0, 0

        new_wallets = total_wallets - prev_wallets
        new_xp = int(total_xp) - int(prev_xp)

        cursor.execute("""
            INSERT INTO platform_stats
                (snapshot_date, total_wallets, total_xp, new_wallets, new_xp)
            VALUES (%s,%s,%s,%s,%s)
            ON DUPLICATE KEY UPDATE
                total_wallets=VALUES(total_wallets),
                total_xp=VALUES(total_xp),
                new_wallets=VALUES(new_wallets),
                new_xp=VALUES(new_xp)
        """, (snapshot_date, total_wallets, total_xp, new_wallets, new_xp))

        conn.commit()
        print(f"âœ… {snapshot_date} å¹³å°ç»Ÿè®¡å·²æ›´æ–° | æ€»é’±åŒ…={total_wallets}, æ–°å¢é’±åŒ…={new_wallets}")

    except Exception as e:
        conn.rollback()
        print(f"âŒ æ›´æ–° {snapshot_date} å¹³å°ç»Ÿè®¡å¤±è´¥: {e}")
    finally:
        cursor.close()
        conn.close()

# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    json_file = "20250928_leaderboard.json"  # æ–‡ä»¶åè¡¨ç¤ºå½“å¤©å¿«ç…§

    base_name = os.path.basename(json_file).split("_")[0]  # 20250903
    record_date = datetime.strptime(base_name, "%Y%m%d").date()

    bulk_insert(json_file)

    platform_date = record_date - timedelta(days=1)
    update_platform_stats(platform_date)

    print(f"ğŸ‰ {record_date} å•å¤©å¢é‡æ•°æ® & {platform_date} å¹³å°ç»Ÿè®¡å®Œæˆ")
